<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="MMLongBench-Doc">
  <meta name="keywords" content="multimodal chatbot">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MMLongBench-Doc</title>

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.1/css/bulma.min.css">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="static/images/logo.png">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/js/all.min.js"></script>
  <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/3.27.0/gradio.js"></script>
</head>


<style>
    .section {
    margin-bottom: -30px; /* Adjust this value as needed to reduce the space */
  }
  .expandable-card .card-text-container {
    max-height: 200px;
    overflow-y: hidden;
    position: relative;
  }

  .expandable-card.expanded .card-text-container {
    max-height: none;
  }

  .expand-btn {
    position: relative;
    display: none;
    background-color: rgba(255, 255, 255, 0.8);
    /* margin-top: -20px; */
    /* justify-content: center; */
    color: #510c75;
    border-color: transparent;
  }

  .expand-btn:hover {
    background-color: rgba(200, 200, 200, 0.8);
    text-decoration: none;
    border-color: transparent;
    color: #510c75;
  }

  .expand-btn:focus {
    outline: none;
    text-decoration: none;
  }

  .expandable-card:not(.expanded) .card-text-container:after {
    content: "";
    position: absolute;
    bottom: 0;
    left: 0;
    width: 100%;
    height: 90px;
    background: linear-gradient(rgba(255, 255, 255, 0.2), rgba(255, 255, 255, 1));
  }

  .expandable-card:not(.expanded) .expand-btn {
    margin-top: -40px;
  }

  .card-body {
    padding-bottom: 5px;
  }

  .vertical-flex-layout {
    justify-content: center;
    align-items: center;
    height: 100%;
    display: flex;
    flex-direction: column;
    gap: 5px;
  }

  .figure-img {
    max-width: 100%;
    height: auto;
  }

  .adjustable-font-size {
    font-size: calc(0.5rem + 2vw);
  }

  .chat-history {
    flex-grow: 1;
    overflow-y: auto;
    /* overflow-x: hidden; */
    padding: 5px;
    border-bottom: 1px solid #ccc;
    margin-bottom: 10px;
  }

  #gradio pre {
    background-color: transparent;
  }
  
	/* ‰ΩøÁî®Ê∏êÂèòÈ¢úËâ≤ÂÆûÁé∞ÂΩ©ËôπÂ≠ó‰Ωì */
	.rainbow-text {
	  background: linear-gradient(to right, #3498db, #2ecc71);
	  -webkit-background-clip: text;
	  color: transparent;
	  display: inline-block;
	  font-weight: bold;
	}
  
</style>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"> <span class="rainbow-text">MMLongBench-Doc</span>: Benchmarking Long-context Document Understanding with Visualizations</h1>
          <div class="is-size-5 publication-authors">
              <span class="author-block"> <a href="">Yubo Ma</a><sup>1</sup>,</span>
              <span class="author-block"> <a href="">Yuhang Zang</a><sup>&dagger;2</sup><sup>,</span>
              <span class="author-block"> <a href="">Liangyu Chen</a><sup>1</sup>,</span>
              <span class="author-block"> <a href="">Meiqi Chen</a><sup>3</sup>,</span>
              <span class="author-block"> <a href="">Yizhu Jiao</a><sup>4</sup>, </span>
              <span class="author-block"> <a href="">Xinze Li</a><sup>1</sup>, </span>
              <span class="author-block"> <a href="">Xinyuan Lu</a><sup>5</sup>, </span>
              <span class="author-block"> <a href="">Ziyu Liu</a><sup>6</sup>, </span>
              <span class="author-block"> <a href="">Yan Ma</a><sup>7</sup>, </span>
              <br>
              <span class="author-block"> <a href="">Xiaoyi Dong</a><sup>2</sup>, </span>
              <span class="author-block"> <a href="">Pan Zhang</a><sup>2</sup>, </span>
              <span class="author-block"> <a href="">Liangming Pan</a><sup>8</sup>, </span>
              <span class="author-block"> <a href="">Yu-Gang Jiang</a><sup>9</sup>, </span>
              <span class="author-block"> <a href="">Jiaqi Wang</a><sup>2</sup>, </span>
              <span class="author-block"> <a href="">Yixin Cao</a><sup>9</sup>, </span>
              <span class="author-block"> <a href="">Aixin Sun</a><sup>1</sup> </span>
            </div>

          <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>S-Lab, Nanyang Technological University</span>
              <span class="author-block"><sup>2</sup>Shanghai AI Laboratory</span>
              <span class="author-block"><sup>3</sup>Peking University</span>
              <span class="author-block"><sup>4</sup>University of Illinois Urbana-Champaign</span>
              <span class="author-block"><sup>5</sup>University of Illinois Urbana-Champaign</span>
              <span class="author-block"><sup>6</sup>Wuhan University</span>
              <span class="author-block"><sup>7</sup>Singapore Management University</span>
              <span class="author-block"><sup>8</sup>University of California, Santa Barbara</span>
              <span class="author-block"><sup>9</sup>Fudan University</span>
          </div>
		  <div class="is-size-6 publication-authors">
              <span class="author-block"><sup>&dagger;</sup>Corresponding authors.</span>
            </div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block"> <a href=""
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="ai ai-arxiv"></i> </span> <span>arXiv</span> </a> </span>
              <!-- Code Link. -->
              <span class="link-block"> <a href="https://github.com/mayubo2333/MMLongBench-Doc"
                   class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="fab fa-github"></i> </span> <span>Code</span> </a> </span>
              <!-- HuggingFace Link. -->
              <span class="link-block"> <a href="https://huggingface.co/datasets/yubo2333/MMLongBench-Doc"
                   class="external-link button is-normal is-rounded is-dark"><span class="icon">ü§ó</span><span>Space</span> </a></span>
              <!-- Dataset Link. -->
             <!--  <span class="link-block"> <a href="https://huggingface.co/spaces/Zery/Alpha-CLIP_LLaVA-1.5" -->
             <!--       class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="far fa-images"></i> </span> <span>Alpha-CLIP+LLM Demo</span></a></span> -->
	      <!-- Dataset Link. -->
              <!-- <span class="link-block"> <a href=""> -->
              <!--      class="external-link button is-normal is-rounded is-dark"> <span class="icon"> <i class="far fa-images"></i> </span> <span>Alpha-CLIP+ImgVar Demo</span></a></span></div>
          	 </div>
       	   </div>
        </div>
  </div>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
	<div style="text-align: center;">
		<!-- <img id="teaser" width="100%" src="static/images/teaser.png"> -->
	  </div><br>
	 
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
		  <style>
			/* ‰ΩøÁî®Ê∏êÂèòÈ¢úËâ≤ÂÆûÁé∞ÂΩ©ËôπÂ≠ó‰Ωì */
			.rainbow-text {
			  background: linear-gradient(to right, #3498db, #2ecc71);
			  -webkit-background-clip: text;
			  color: transparent;
			  display: inline-block;
			  font-weight: bold;
			}
		  </style>
          <p>
            Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, including single-page document understanding (DU). However, their abilities on long-context DU abilities remain an open problem due to the lack of related benchmarks. This work presents MMLongBench-Doc, a long-context, multi-modality benchmark constructed upon 130 lengthy documents with an average of 49.4 pages and 20,971 tokens. It incorporates 1,062 expert-annotated questions and evaluates LVLMs' long-context DU abilities from diverse aspects: information identification (44.0% single-page question), cross-page comprehension (33.2% cross-page question) and hallucination severity (22.8% unanswerable question). Towards comprehensive evaluation, these questions cover diverse evidence sources (i.e., text, image, chart, table, layout structure) and locations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing GPT-4o achieves only a 42.7% F1 score, while the second-best GPT-4V scores 31.4%. Furthermore, most LVLMs even present worse performance than single-modality LLMs which are fed with OCR-parsed, lossy documents. These results validate the necessity of future research toward better long-context LVLMs for this task.
          </p>
      </div>
    </div>
    <!--/ Abstract. -->

<!--     Paper video. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/UAUJNFJSbiI?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>


<section class="section"  style="background-color:#efeff081" id="Highlight">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-six-fifths">
            <h2 class="title is-3">üî•Highlight</h2>
            <div class="content has-text-justified">
              <p style="font-size: 15px;">
                <ul>
                <li><b>Diverse Domain:</b>This benchmark is constructed upon 130 documents, including Research Report, Financial Report, Academic Paper, Brochure, Guideline, Administration & Industry File and Tutorial / Workshop.</li>
                <li><b>Long Context:</b> Each document has an average of 49.4 pages and 20,971 tokens.</li>
                <li><b>Multi-modality:</b>The questions are designed from various, multi-modal sources in the documents, including text, layout structure, table, chart and image.</li>
                </ul>
              </p>
            </div>
          </div>
        </div>
      </div>
</section><br>

<section class="section" id="Benchmark Overview">
   <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> <span class="rainbow-text">MMLongBench-Doc</span> Overview</h2>
        </div>
	    </div>
	      <div class="container is-max-desktop">
	        <div class="columns is-centered">
	          <div class="column is-full-width">
	            <div class="content has-text-justified">
	              <p>
                  We construct MMLONGBENCH-DOC to evaluate the understanding capabilities of Large Vision-Lanaguage Models (LVLMs) on long-context, multi-modality documents. This benchmark targets document understanding (DU), which is a long-standing task in urgent and practical needs. Most previous datasets on DU focus on single-page DU. There lacks a unified, high-quality benchmark that (1) includes diverse lengthy documents and questions with detailed meta-data annotations and (2) evaluates how LVLMs perform on lengthy documents with multi-modality components (e.g., plain text, table, chart, image). Our benchmark is constructed to bridge such a gap. <br>
                    <centering>
	                  <div style="text-align: center;">
	                    <img id="pipeline" width="100%" src="static/images/top_figure.png">
	                  </div>
                  </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>

<!-- <section class="section" id="Benchmark Construction">
   <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> <span class="rainbow-text">MMDU</span> Construction</h2>
        </div>
	    </div>
	      <div class="container is-max-desktop">
	        <div class="columns is-centered">
	          <div class="column is-full-width">
	            <div class="content has-text-justified">
	              <p>
                      This is an overview of <b>(a) data preparation</b> and <b>(b) generation pipeline</b> for <span class="rainbow-text">MMDU</span> and <span class="rainbow-text">MMDU-45k</span>. We first collect the relevant image and text descriptions from Wikipedia using the clustering algorithm. Then we prompt GPT-4o to design multi-turn questions. The human annotators revise the GPT-4o response as the ground-truth answers.
                      <centering>
	                  <div style="text-align: center;">
	                    <img id="pipeline" width="100%" src="static/images/pipeline.png">
	                  </div> 
                  </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section> -->


<section class="section" id="Evaluation">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> <span class="rainbow-text">MMLongBench-Doc</span> Evaluation</h2>
        </div>
	    </div>
	      <div class="container is-max-desktop">
	        <div class="columns is-centered">
	          <div class="column is-full-width">
	            <div class="content has-text-justified">
	              <p>
 
                Our key findings are summarized as follows. (1) The performance demonstrates that long-context document understanding is still a challenging and unsolved task for current LVLMs. The best-performing LVLM, GPT-4o, merely achieves 42.7% F1 score. The second best-performing LVLM, GPT-4V, lags behind by over 10% percent and presents 31.4% F1 score. All other LVLMs only achieve about 20% or even lower F1 scores. (2) Though far from satisfactory, GPT-4o performs much better than all other models (including GPT-4V). Thus we speculate that the multi-modal pre-training paradigm significantly benefits LVLMs' cross-modality understanding capabilities. (3) The performances of different models are highly correlated with their acceptable image numbers and maximum image resolutions. Notably, open-source LVLMs that support high-resolution images exhibit superior performance compared to those with lower resolution limits, owing to our concatenation pre-processing operator.
                Surprisingly and frustratingly, LVLMs even demonstrate overall worse performance than LLMs on long-context DU abilities, even LLMs are fed with lossy OCR-parsed documents. Specifically, Gemini-1.5-Pro and Claude-3 Opus have 4.3% and 6.5% absolute F1-score degradations on vision versions. And the best-performing LLM (Mixtral) also surpasses the best-performing LVLM (InternVL-v1.5) by 11.9%. The above results clearly reveal that most current LVLMs are still not proficient in cross-modality, long-context document understandings. It is comforting that GPT-4o and GPT-4-turbo achieve better performance when seeing multi-modality PDF documents than parsed text by 10.8% and 4.4% F1-score, respectively. Their performances validate the feasibility, benefit, and necessity of understanding documents in an end-to-end, cross-modality approach. We speculate that the scarce related pre-training corpus (i.e., extremely multi-image or lengthy documents) hinders the long-context DU capabilities of other LVLMs. We will leave related explorations for future work. 

                    <centering>
	                  <div style="text-align: center;">
	                    <img id="teaser" width="100%" src="static/images/eval_result.png">
	                  </div>
	              </p>
	            </div>
	            </b></font>
        </div>
      </div>
    </section>

<section class="section" id="Results on MMDU">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Case Study for <span class="rainbow-text">MMLongBench-Doc</span> </h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
<!-- Our key findings are summarized as follows. (1) Our benchmark poses significant challenges to current LVLMs. Notably, even the advanced GPT-4o model achieves an average accuracy of only 70.2%, while open-source LVLMs achieve merely 42.8% or lower, indicating substantial room for improvement. (2) We observe a significant performance gap between closed-source LVLMs and open-source LVLMs. We speculate that this disparity arises from the scarcity of open-source instruction tuning data with multi-turn and multi-image capabilities, leading to limited improvement in open-source LVLMs. This inspired us to collect and release <span class="rainbow-text">MMDU-45k</span>, a valuable resource for the open-source community, to bridge this gap. -->
                  <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/case_study.png">
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section>

<!-- <section class="section" id="Alpha-CLIP pipeline">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP Pipeline</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
                The pipeline of our <b>data generation method</b> and <b>model architecture</b>. (a) Our method generates millions of RGBA region-text pairs. (b) Alpha-CLIP modifies the CLIP image encoder to take an additional alpha channel along with RGB. We first generate millions of RGBA region-text data from grounding and classification datasets. Using our generated data, we then train our Alpha-CLIP with additional Alpha-channel inputs.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/pipeline.png">     
                  </div>
              </p>
            </div>
            </b></font>
          </div>
        </div>
      </div>
    </section> -->


<!-- <section class="section" id="Alpha-CLIP Attn">
      <div class="columns is-centered has-text-centered">
        <div class="column is-six-fifths">
          <h2 class="title is-3"> Alpha-CLIP Attention Map Visualization</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-full-width">
            <div class="content has-text-justified">
              <p>
			  	We check the attention map of [CLS] token in the last transformer block in the vision encoder.
Each first line per four is from original CLIP and the other three lines are from Alpha-CLIP with user-defined focus regions marked
in red. This visualization verifies that Alpha-CLIP <b>pays more attention to the area to focus on</b> and more importantly, with <b>no damage to the 2D location information</b> preserved in the feature location of the original CLIP.
                <centering>
                  <div style="text-align: center;">
                    <img id="teaser" width="100%" src="static/images/attn.png">     
                  </div>
              </p>
            </div>
            </b></font>
			</div>
			
          </div>
        </div>
      </div>
    </section> -->



<!-- <section class="section" id="BibTeX">
      <div class="container is-max-desktop content">
        <h2 class="title">BibTeX</h2>
        <pre><code>
          @misc{sun2023alphaclip,
            title={Alpha-CLIP: A CLIP Model Focusing on Wherever You Want}, 
            author={Zeyi Sun and Ye Fang and Tong Wu and Pan Zhang and Yuhang Zang and Shu Kong and Yuanjun Xiong and Dahua Lin and Jiaqi Wang},
            year={2023},
            eprint={2312.03818},
            archivePrefix={arXiv},
            primaryClass={cs.CV}
          }
      </code></pre>
      </div>
    </section> -->

				 
				 
<footer class="footer">
  <div class="container">
  	<!-- link
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2210.04150.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/facebookresearch/ov-seg" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
	-->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
<!--           <p>
            Thanks to <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> and <a href="https://jerryxu.net/GroupViT">GroupViT</a>.
          </p> -->
        </div>
<!-- 	<div style="width: 30%; text-align: center;">
              <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=o6auWXiSftSiyKivQFuM8x7SJSr5FX15LEUTL1Uy3ic&cl=ffffff&w=a"></script>
        </div> -->
      </div>
    </div>
  </div>
    <!-- <div style="width: 30%; text-align: center;">
        <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=0FUg0lO4os5unZqGRpjtXrtArD4RG9IPorExFM578F0&cl=ffffff&w=a"></script>
    </div> -->


</footer>


</body>
</html>
